All experiments were conducted on an in-house cluster equipped with two Nvidia Quadro RTX 8000 GPUs with 48 GB of memory each. The FL experiments each ran 20 communication rounds with 15 local training epochs per round. The reported metrics represent averages calculated over these 20 rounds, unless specified otherwise. The centralized experiment trained for 100 epochs. To simulate a moderately non-IID data distribution among clients, we used a Dirichlet distribution~\cite{hsu2019measuring} with an alpha parameter of $0.1$. Unless specified otherwise, data was distributed across 10~clients with a participation ratio of $0.8$.

We utilize the SGD optimizer with a learning rate of $0.001$ for all experiments. Local training is conducted using a batch size of 16. Clustering-based approaches assign each client 16 local clusters, whereas methods that used global clustering had 128 global clusters assigned. The EMA parameter use a decay value of $0.9$ and a temperature value of $0.1$ for all objectives.

Our implementation was developed in PyTorch and uses the Flower framework for federated learning. We employ a Multi-Layer Perceptron (MLP) as the backbone model to minimize communication costs and improve training speed. The model consists of two linear layers with output dimensions of 300 and 200, each followed by a ReLU activation layer. Projector and predictor architectures were designed based on the original reference papers, with Orchestra using a two-layer structure for both projector and predictor. We repeated each experiment using three different random seed values and reported the average results.